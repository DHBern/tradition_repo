%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The Legrand Orange Book
% LaTeX Template
% Version 2.0 (9/2/15)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Mathias Legrand (legrand.mathias@gmail.com) with modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
% Compiling this template:
% This template uses biber for its bibliography and makeindex for its index.
% When you first open the template, compile it from the command line with the 
% commands below to make sure your LaTeX distribution is configured correctly:
%
% 1) pdflatex main
% 2) makeindex main.idx -s StyleInd.ist
% 3) biber main
% 4) pdflatex main x 2
%
% After this, when you wish to update the bibliography/index use the appropriate
% command above and make sure to compile with pdflatex several times 
% afterwards to propagate your changes to the document.
%
% This template also uses a number of packages which may need to be
% updated to the newest versions for the template to compile. It is strongly
% recommended you update your LaTeX distribution if you have any
% compilation errors.
%
% Important note:
% Chapter heading images should have a 2:1 width:height ratio,
% e.g. 920px width and 460px height.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[11pt,fleqn,openany]{book} % Default font size and left-justified equations

%----------------------------------------------------------------------------------------

\input{structure} % Insert the commands.tex file which contains the majority of the structure behind the template

\begin{document}
%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\begingroup
\thispagestyle{empty}
\begin{tikzpicture}[remember picture,overlay]
\coordinate [below=12cm] (midpoint) at (current page.north);
\node at (current page.north west)
{\begin{tikzpicture}[remember picture,overlay]
\node[anchor=north west,inner sep=0pt] at (0,0) {\includegraphics[width=\paperwidth]{Pictures/background.pdf}}; % Background image
\draw[anchor=north] (midpoint) node [fill=ocre!30!white,fill opacity=0.6,text opacity=1,inner sep=1cm]{\Huge\centering\bfseries\sffamily\parbox[c][][t]{\paperwidth}{\centering Stemmarest\\[15pt] % Book title
{\Large Documentation of the PSE2 Project 2015}\\[20pt] % Subtitle
{\small Ido Gershoni, Ramona Imhof, Joel Niklaus, Jakob Schaerer, Severin Zumbrunn}}}; % Author name
\end{tikzpicture}};
\end{tikzpicture}
\vfill
\endgroup

%----------------------------------------------------------------------------------------
%	COPYRIGHT PAGE
%----------------------------------------------------------------------------------------

\newpage
~\vfill
\thispagestyle{empty}

\noindent Copyright \copyright\ 2015 Team PSE2\\ % Copyright notice

\noindent \textsc{Published by ...}\\ % Publisher

\noindent \textsc{....org}\\ % URL

\noindent Licensed under the Creative Commons Attribution-NonCommercial 3.0 Unported License (the ``License''). You may not use this file except in compliance with the License. You may obtain a copy of the License at \url{http://creativecommons.org/licenses/by-nc/3.0}. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \textsc{``as is'' basis, without warranties or conditions of any kind}, either express or implied. See the License for the specific language governing permissions and limitations under the License.\\ % License information

\noindent \textit{First printing, March 2013} % Printing/edition date

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

\chapterimage{header.png} % Table of contents heading image

\pagestyle{empty} % No headers

\tableofcontents % Print the table of contents itself

\cleardoublepage % Forces the first chapter to start on an odd page so it's on the right

\pagestyle{fancy} % Print headers again

%----------------------------------------------------------------------------------------
%	PART
%----------------------------------------------------------------------------------------

\part{Project}

%----------------------------------------------------------------------------------------
%	CHAPTER 1
%----------------------------------------------------------------------------------------

\chapterimage{header.png} % Chapter heading image

\chapter{Introduction}

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.

%----------------------------------------------------------------------------------------
%	CHAPTER 
%----------------------------------------------------------------------------------------

\chapterimage{header.png} % Chapter heading image

\chapter{Setup}

\section*{Downloading}

git clone https://github.com/tohotforice/PSE2\_DH.git

\section*{Building}

Stemmarest needs to be built using Maven. This can be done using a java IDE (e.g Eclipse) and a Maven plugin

\section*{Running}

As this application represents a server side only, there is no full GUI included. It is possible though to test it by using the test interface testGui.html which is located at StemmaClient.

\section*{Using the test interface}

\begin{itemize}
\item Create a user and give it an id (this is necessary as every graph needs to be owned by a user)
\item Import an GraphML file using the id of the user you have created. The generated id of the tradition will be returned
\item Use the custom request by typing in the API call you want (all calls are listed in the documentation)
\end{itemize}
A word about node id's: when a graph is being imported each node gets from Neo4j a unique id-number. In order to use an id in an API call (e.g. reading-id) it is necessary to explicitly get it from the data base. This can be done by using the getAllReadings method (getallreadings/fromtradition/{traditionId}) or by actually going into the data base (see Neo4j visualization in section Database)

%----------------------------------------------------------------------------------------
%	CHAPTER 2
%----------------------------------------------------------------------------------------

\chapterimage{header.png} % Chapter heading image

\chapter{Database (neo4j)}

In our project we used a graph database instead of a relational database.\\ This was due to the fact, that relational databases are much faster when it comes down to look for objects in a list that fulfill some constraints to other objects. Normally in a relational database you would use multiple joins to get your desired result. In the graph way it’s much easier because you traverse the graph (previously mentioned as list) from top down using either breadth- or depth-first algorithm and look for a specific relation between two nodes (previously objects). In respect of that we save a lot of time traversing instead of writing complicated queries which run mostly slow. \\ \quad \\Therefore we use Neo4J (Additional info can be found under http://neo4j.com/developer/graph-db-vs-rdbms/). Neo4J is a graph database that is capable of managing millions of nodes and relationships and returning or changing them within logarithmic or even constant time. That means that it won’t make a big difference whether you use 10 nodes, or 1 million. You can find additional information on benchmarking in the related chapter of this documentation. \\ \quad \\Our database is mainly one big graph. We use different Labels for Nodes and Relationships to increase the search speed. \\ \quad \\ In the database we use only a few labels:\\ \quad \\
\begin{tabular}{|l|l|}
\hline 
\textbf{Nodes} & \textbf{Relationships} \\ 
\hline 
ROOT & RELATIONSHIP \\ 
\hline 
STEMMA & STEMMA \\ 
\hline 
WITNESS & NORMAL \\ 
\hline 
TRADITION &  \\ 
\hline 
USER &  \\ 
\hline 
WORD &  \\ 
\hline 
\end{tabular} \\
Since each label is stored in another file, searching or traversing the graph is stunningly fast. \\ \quad \\ The database structure is as follows:
\begin{center}
\includegraphics[scale=.5]{Pictures/database.png} 
\end{center} 
Neo4J is delivered with a powerful script language called cypher. It’s the equivalent to SQL in relational databases. Cypher is a declarative graph query language that allows for expressive and efficient querying and updating of the graph store. Using cypher you can write simple queries that return nodes or traverse graphs. Cypher queries can be sent to the database and will then be interpreted and translated into an execution plan by the ExecutionEngine. This takes some time and therefore cypher can not compete with the native java traversal API.

%----------------------------------------------------------------------------------------
%	CHAPTER 3
%----------------------------------------------------------------------------------------

\chapterimage{header.png} % Chapter heading image

\chapter{Jersey}

In our project we built a REST-API for stemmaweb an online tool for textual scholars willing to explore their texts. For this REST-API we needed a simple and easy to use Java framework and chose Jersey. \\
Jersey is an open source framework for developing RESTful Web Services in Java that is built upon JAX-RS and serves as a JAX-RS Reference Implementation. It adds additional features and utilities in order to further simplify development of REST-APIs. It abstracts away low-level details of the client-server communication which made it more easy for us to concentrate on the actual implementation of the user stories. In our project it is deployed with GlassFish, a Java EE Application Server. Jersey can help support exposing the data in very different media types, including JSON, which we used very frequently. \\
Jersey uses Annotations which made it very easy to use for us.\\
Here as an example the method declaration of duplicateReading:
\begin{lstlisting}
   @POST
   @Path("duplicatereading/fromtradition/{tradId}")
   @Consumes(MediaType.APPLICATION_JSON)
   @Produces(MediaType.APPLICATION_JSON)
   public Response duplicateReading(DuplicateModel duplicateModel)
\end{lstlisting}
At the top the „POST“ annotation states the http method. Then the „Path“ annotation lets us set the url path with parameters in curly braces. Furthermore the method can „consume“ data, in this case JSON. The „DuplicateModel“ is passed with the call as a JSON object and then gets parsed in a POJO. Last but not least the method „produces“ a response, in this case also in JSON. 


%----------------------------------------------------------------------------------------
%	CHAPTER 4
%----------------------------------------------------------------------------------------

\chapterimage{header.png} % Chapter heading image

\chapter{Performance}

One of the main goals of the redesign is it to create a faster RESTful service. To assure the speed of the service some performance tests are done. 
\\ \quad \\
The aim of the performance tests is it to show that the response time of the service is limited and within a usable range. So the time to execute all operations for a certain request is measured. This contains the time to transmit the data over HTTP, the time to execute the internal algorithms and the time to access the database. All the Data are transmitted over the local loop interface, so the network speed is not measured. 
\\ \quad \\
To do the tests the database is populated by a random graph which contains several valid traditions on which the REST requests can be executed. Several tests with databases of a different size are done to show that the response time does not change with an increasing database size. (Actually there are several search by ID requests in the database which execute under O(log n) but as the database access time takes such a small percentage of the measurement scope this does not show up in the size of database expected for an operational database.)
\\ \quad \\
In the following measurement series the dimension of the database size is tested. This tests show, that the RESTful service response time is not influenced by the size of the database in a significant way. This is related to the use of the Graphdatabase which allows to work on a subgraph without filtering the whole database. 
\begin{remark}
The implementation of stemma rest uses some search node by id methods which search over the complete database but those are in $O(log n)$ and are not seen in the noise of the other operations. In a much bigger Database those methods will slow down the REST requests. But it is not expected that the database will grow that big, that this operations will have any impact.
\end{remark} 

\begin{figure}[h!]
  \caption{Database with 1000 nodes,  working tradition with 100 nodes }
  \centering
    \includegraphics[scale=0.45]{1knode.png}
\end{figure}

\begin{figure}[h!]
  \caption{Database with 100000 nodes,  working tradition with 100 nodes }
  \centering
    \includegraphics[scale=0.45]{100knode.png}
\end{figure}

\begin{figure}[h!]
  \caption{Database with 1000000 nodes,  working tradition with 100 nodes }
  \centering
    \includegraphics[scale=0.45]{1000knode.png}
\end{figure}
The previous measurement series tested the dimension of the database size and could show that the response time is almost independent of it for the expected database size of something between 1000 and 1 Million Nodes (Readings). This independence results out of the fact, that each Tradition can be selected as a subgraph and the algorithms only have a subset of the whole database to search trough. It is obviously that those algorithms are not independent of the tradition size as the working subset grows with a bigger tradition. Most of the algorithms to work on a tradition are in $O(log n)$ but there are also some export and import functions which have to handle each node and relation of the tradition and run in $O(n)$. 
\\ \quad \\
In the following test series the dimension of the tradition size is varied. 
\begin{figure}[h!]
  \caption{Database with 10000 nodes,  working tradition with 1000 nodes }
  \centering
    \includegraphics[scale=0.45]{trad1kwords.png}
\end{figure}
\begin{figure}[h!]
  \caption{Database with 10000 nodes,  working tradition with 10000 nodes }
  \centering
    \includegraphics[scale=0.45]{trad10kwords.png}
\end{figure}
This measurement shows that the execution time depend on the size of the working tradition. In this test result the getAllReadingsFromATradition each reading of the tradition is parsed to a JSON Object and all are returned as an List. The parsing of those nodes executes in O(n) and the downloading of the JSON file takes also its time. As larger traditions are not expected the execution time of those methods is in the accepted range.

%----------------------------------------------------------------------------------------
%	PART
%----------------------------------------------------------------------------------------

\part{Testing}

%----------------------------------------------------------------------------------------
%	CHAPTER 5
%----------------------------------------------------------------------------------------

\chapterimage{header.png} % Chapter heading image

\chapter{Concept}
This chapter describes the test-concept of the Digital Humanities PSE2 Project. The testing is used to assure the quality of the project and for test driven development. All tests are written in a manner they don't have any impact on the architecture of the project. 

\section*{Integration Tests}
Every user story is tested by an integration Test. The integration tests assures the quality of the project. The technique of integration tests is described in the Integration test chapter. 

\section*{Unit Tests}
Unit tests are used for test driven development and are defined by the developer. They are only for the development process and are not referenced in quality audits.

\newpage

\section*{Jersey Overview} 

\begin{center}
\includegraphics[scale=.40]{Pictures/jerseyoverview.png} 
\end{center}

In the productive system for every REST call jersey is instantiating the requested resource and providing the service. Each resource object has its own database service, which is closed after each call. In production a embedded neo4j GraphDatabase is used. To achieve a minimal invasive test system the productive database needs to be replaced with a test database. To change the database with minimal test related code in the project, the GraphDatabaseServiceProvider can be configured to return an impermanent Database.

%----------------------------------------------------------------------------------------
%	CHAPTER 6
%----------------------------------------------------------------------------------------

\chapter{Configure the Test-Database}

In this Project the GraphDatabaseService is a Singleton Object provided by the GraphDatabaseServiceProvider. To use a Testdatabase in the Tests the following steps have to be done.
A Claswide GraphDatabaseService object db has to be registered.

\begin{lstlisting}[language=java]
    GraphDatabaseService db;
\end{lstlisting}
In the @Before method the singleton GraphDatabaseService provided by the GraphDatabaseServiceProvider has to be overwritten by the following line:

\begin{lstlisting}[language=java]
    GraphDatabaseServiceProvider.setImpermanentDatabase();
\end{lstlisting}
Later the db object can be initialized by:
\begin{lstlisting}[language=java]
    db = new GraphDatabaseServiceProvider().getDatabase();
\end{lstlisting}
In the @After method the database has to be closed
\begin{lstlisting}[language=java]
    db.shutdown();
\end{lstlisting}
With this configuration the impermanent Testdatabase of Neo4j is used during the tests. 
%----------------------------------------------------------------------------------------
%	CHAPTER 7
%----------------------------------------------------------------------------------------

\chapterimage{header.png} % Chapter heading image

\chapter{Unit Test}

\begin{center}
\includegraphics[scale=.4]{Pictures/junitoverview.png}
\end{center} 
 
For Unit Tests the methods of the resource are called directly.
\begin{lstlisting}[language=java]
@Test
public void SimpleTest(){
  String actualResponse = userResource.getIt();
  assertEquals(actualResponse, "User!");
}
\end{lstlisting}


\subsection*{Example}
\url{https://github.com/tohotforice/PSE2_DH/blob/e364fcb0c164981281c5799a6bf9f9f9ea5eb503/stemmarest/src/test/java/net/stemmaweb/stemmaserver/UserUnitTest.java}
%----------------------------------------------------------------------------------------
%	CHAPTER 8
%----------------------------------------------------------------------------------------

\chapterimage{header.png} % Chapter heading image

\chapter{Integration Tests}

\begin{center}
\includegraphics[scale=.4]{Pictures/jerseytestoverview.png} 
\end{center}

To inject objects into a resource it is mandatory that the resource is created statically at the place the injection is done. This is not possible when the resources are instantiated when a REST call occurs. To solve this JerseyTestServerFactory creates a server where already instantiated resources can be registered. 
To start a JerseyTestServer a global JerseyTest has to be created.
\begin{lstlisting}[language=java]
private JerseyTest jerseyTest;
\end{lstlisting}
The JerseyTestServerFactory creates a JerseyTest with already instantiated resources. This is necessary to inject the mock objects. Multiple resources can be added by chaining .addResource(..).addResource()
\begin{lstlisting}[language=java]
jerseyTest = JerseyTestServerFactory.newJerseyTestServer()
	.addResource(userResource).create();
jerseyTest.setUp();
\end{lstlisting}

The test is done by calling a webresource of jerseyTest
\begin{lstlisting}[language=java]
@Test
public void SimpleTest(){
  String actualResponse = jerseyTest.resource()
      .path("/user").get(String.class);
  assertEquals(actualResponse, "User!");
}
\end{lstlisting}

\subsection*{Example}
\url{https://github.com/tohotforice/PSE2_DH/blob/e364fcb0c164981281c5799a6bf9f9f9ea5eb503/stemmarest/src/test/java/net/stemmaweb/stemmaserver/UserTest.java}

%----------------------------------------------------------------------------------------
%	CHAPTER 9
%----------------------------------------------------------------------------------------

\chapterimage{header.png} % Chapter heading image

\chapter{Benchmark Testing}

A main goal of the PSE2 stemmarest project is a good performance compared to the previous RESTful service. To measure the performance benchmark testing is needed. A benchmark test basically calls the RESTful service multiple times and measure the response time. To achieve this \textit{com.carrotsearch.junitbenchmarks} a handy JUnit benchmark test suite is used. This JUnitbenchmarks measure the time which is used to execute a test and can generate visual representations of the measurement.\\
For the benchmark testing it is of interest to have a variety of different databases. Those databases should differ in their size from small to very huge. This allows to measure the algorithms in extreme situations.
To generate valid graphs only limited by diskspace the class \textit{RandomGraphGenerator} can be used. By calling the static method role a graph is generated according to the parameters.\\
\begin{remark}
Please note that the response time highly depends on the hardware the tests are running on and the actual state of Javas virtual machine. 
\end{remark}
To reduce the influence of the virtual machine before the measurements 5 warm-up calls are done to bring the virtual machine to live. The hardware which was used for testing is represented in the report.

\section*{Setup}
All the classes related to the Benchmark Tests can be found in the package \\ \textit{net.stemmaweb.stemmaserver.benachmarktests}. The class BenchmarkTests contains all Tests. The classes Benchmark<n>Nodes contain the database generation. Here is configured how many nodes the database has. This are also the classes which are run with JUnit test. BenchmarkTests cant be run as a JUnit test as it is a abstract class. s

\begin{center}
\includegraphics[scale=0.4]{BenchmarkTestsClassOverview.jpg} 
\end{center}

Tests are simply implemented in the BenchmarkTests class with the @Test annotation. It is best practice only to implement the restcall in this method and only test if Response.Status is OK. This assures that as less as possible overhead time is measured. And the integration- and JUnit tests should be done on a other place.

\begin{remark}
JUnitBenchmarks measures the time to execute (@Before, @Test, @After). Heavy operations which should not be measured can be done in @BeforeClass and @AfterClass.
\end{remark}

To create a new database test-environment copy the class Benchmark600Nodes and rename it to the count of Nodes that should be inserted. In the class itself only two small adjustments need to be done. First change the name of the report file \textit{@BenchmarkMethodChart(filePrefix = ''benchmark/benchmark-600Nodes'')}. Second adjust the properties of the database which should be generated \textit{rgg.role(db, 2, 1, 3, 100);}. role(databaseService, cardinalityOfUsers, cardinalityOfTraditionsPerUser, cardinalityOfWitnessesPerTradition, degreeOfTheTraditionGraphs)

\section*{Run Benchmarktests}
The Benchmarktests can be run as every JUnit test. But to generate the report an argument needs to be passed by. 
Create a JUnit Test as follows:

\begin{center}
\includegraphics[scale=0.4]{benchmarkTest.png} 
\end{center}

\begin{center}
\includegraphics[scale=0.4]{benchmarkTestArgument.png} 
\end{center}

On the tab Arguments \textit{-Djub.consumers=CONSOLE,H2 -Djub.db.file=.benchmarks} has to be inserted into the VM Arguments input. After the test can be run as usual. After the test execution the reports are stored under \textit{benchmark/}.
 
\begin{remark}
The execution of the tests will take some time because of the generation of huge graphs. Its recommended not to use the computer during this tests.  
\end{remark}


%----------------------------------------------------------------------------------------
%	PART
%----------------------------------------------------------------------------------------

\part{RESTful API}

\include{api_generated}

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\chapter*{Bibliography}
\addcontentsline{toc}{chapter}{\textcolor{ocre}{Bibliography}}
\section*{Books}
\addcontentsline{toc}{section}{Books}
\printbibliography[heading=bibempty,type=book]
\section*{Articles}
\addcontentsline{toc}{section}{Articles}
\printbibliography[heading=bibempty,type=article]

%----------------------------------------------------------------------------------------
%	INDEX
%----------------------------------------------------------------------------------------

\cleardoublepage
\phantomsection
\setlength{\columnsep}{0.75cm}
\addcontentsline{toc}{chapter}{\textcolor{ocre}{Index}}
\printindex

%----------------------------------------------------------------------------------------

\end{document}